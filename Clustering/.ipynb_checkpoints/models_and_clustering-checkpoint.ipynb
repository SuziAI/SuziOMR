{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611e6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage.util import random_noise\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import time\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import random\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca6abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemperatureScalingCalibrationModule(\n",
      "  (model): FashionCNN(\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "    (drop): Dropout2d(p=0.25, inplace=False)\n",
      "    (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "    (fc3): Linear(in_features=120, out_features=12, bias=True)\n",
      "    (logits): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self, num_output_classes):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=num_output_classes)\n",
    "        self.logits = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        print(\"L1\", out)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.logits(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_representation(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class TemperatureScalingCalibrationModule(nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # the single temperature scaling parameter, the initialization value doesn't\n",
    "        # seem to matter that much based on some ad-hoc experimentation\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def get_representation(self, x):\n",
    "        return self.model.get_representation(x)\n",
    "        \n",
    "    def forward_unscaled(self, x):\n",
    "        logits = self.model(x)\n",
    "        scores = nn.functional.softmax(logits, dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def forward(self, x):\n",
    "        scaled_logits = self.forward_logit(x)\n",
    "        scores = nn.functional.softmax(scaled_logits, dim=1)\n",
    "        return scores\n",
    "\n",
    "    def forward_logit(self, x):      \n",
    "        logits = self.model(x)\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def fit(self, data_loader, n_epochs: int = 10, batch_size: int = 64, lr: float = 0.01):\n",
    "        \"\"\"fits the temperature scaling parameter.\"\"\"\n",
    "        assert isinstance(data_loader, DataLoader), \"data_loader must be an instance of DataLoader\"\n",
    "    \n",
    "        print(self.temperature.requires_grad)\n",
    "        \n",
    "        self.freeze_base_model()\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in data_loader:\n",
    "                images, labels, _ = batch  \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "                self.zero_grad()\n",
    "                scaled_logits = self.forward_logit(images)  # Use forward to get scaled logits\n",
    "                loss = criterion(scaled_logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "        return self\n",
    "\n",
    "\n",
    "    def freeze_base_model(self):\n",
    "        \"\"\"remember to freeze base model's parameters when training temperature scaler\"\"\"\n",
    "        self.model.eval()\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "        return self\n",
    "\n",
    "print(TemperatureScalingCalibrationModule(FashionCNN(num_output_classes=12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e9323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_model = TemperatureScalingCalibrationModule(FashionCNN(num_output_classes=11))\n",
    "pitch_model.load_state_dict(torch.load(\"./model_pitch.std\"))\n",
    "\n",
    "secondary_model = TemperatureScalingCalibrationModule(FashionCNN(num_output_classes=7))\n",
    "secondary_model.load_state_dict(torch.load(\"./model_secondary.std\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d61df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_annotation =  {\n",
    "            \"pitch\": {0: \"HE\", 1: \"SI\", 2: \"YI\", 3: \"SHANG\", 4: \"GOU\", 5: \"CHE\", 6: \"GONG\", 7: \"FAN\", 8: \"LIU\", 9: \"WU\", 10: \"GAO_WU\"},\n",
    "            \"secondary\": {0: None, 1: \"DA_DUN\", 2: \"XIAO_ZHU\", 3: \"DING_ZHU\", 4: \"DA_ZHU\", 5: \"ZHE\", 6: \"YE\"}\n",
    "}\n",
    "\n",
    "annotation_to_class = {\n",
    "    \"pitch\": {\"HE\": 0, \"SI\": 1, \"YI\": 2, \"SHANG\": 3, \"GOU\": 4, \"CHE\": 5, \"GONG\": 6, \"FAN\": 7, \"LIU\": 8, \"WU\": 9, \"GAO_WU\": 10},\n",
    "    \"secondary\": {None: 0, \"DA_DUN\": 1, \"XIAO_ZHU\": 2, \"DING_ZHU\": 3, \"DA_ZHU\": 4, \"ZHE\": 5, \"YE\": 6}\n",
    "}\n",
    "\n",
    "## this function takes the path_to_folder (i.e., the folder where the dataset.json is in)\n",
    "## and returns a list of the dataset entries. Each entry consists of the keys:\n",
    "##     \"image_path\": \n",
    "##     \"type\": The type of the box (in our case, this is 'Music' only)\n",
    "##     \"annotation\": The annotation string\n",
    "##     \"image\": The image as uint8 array representation\n",
    "##     \"is_simple\": This is True if the notation is \"simple notation\" as opposed to \"composite notation\"\n",
    "def open_dataset(path_to_folder):\n",
    "    path_to_json = os.path.join(path_to_folder, \"dataset.json\")\n",
    "    with open(path_to_json) as file:\n",
    "        dataset_json = json.load(file)\n",
    "        output_list = []\n",
    "        \n",
    "        for idx in range(len(dataset_json)):\n",
    "            if dataset_json[idx][\"annotation\"][\"pitch\"] is None or dataset_json[idx][\"annotation\"][\"pitch\"] == \"None\":\n",
    "                continue\n",
    "            temp_dict = {}\n",
    "            temp_dict[\"image\"] = cv2.imread(os.path.join(path_to_folder, dataset_json[idx][\"image_path\"]), cv2.IMREAD_GRAYSCALE)\n",
    "            temp_dict[\"is_simple\"] = True if dataset_json[idx][\"annotation\"][\"secondary\"] == None else False\n",
    "            temp_dict[\"annotation\"] = {}\n",
    "            temp_dict[\"annotation\"][\"pitch\"] = annotation_to_class[\"pitch\"][dataset_json[idx][\"annotation\"][\"pitch\"]]\n",
    "            temp_dict[\"annotation\"][\"secondary\"] = annotation_to_class[\"secondary\"][dataset_json[idx][\"annotation\"][\"secondary\"]]\n",
    "            temp_dict[\"edition\"] = os.path.basename(dataset_json[idx][\"image_path\"]).split(\"_\")[0]\n",
    "            temp_dict[\"image_id\"] = \"_\".join(dataset_json[idx][\"image_path\"].split(\"_\")[:-1])\n",
    "            temp_dict[\"image_path\"] = os.path.basename(dataset_json[idx][\"image_path\"])\n",
    "            output_list.append(temp_dict)\n",
    "    return output_list\n",
    "\n",
    "def get_cropped_dataset(dataset):  ## creates a new dataset where each of the images is cropped\n",
    "    \n",
    "    def remove_small_blobs(image):  # remove small isolated connected black areas\n",
    "        noise_removal_threshold = 1  \n",
    "        mask = np.ones_like(image)*255\n",
    "        contours, hierarchy = cv2.findContours(255-image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        for contour in contours:\n",
    "          area = cv2.contourArea(contour)\n",
    "          if area >= noise_removal_threshold:\n",
    "            cv2.fillPoly(mask, [contour], 0)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def crop_excess_whitespace(image):\n",
    "        gray = 255*(image < 128).astype(np.uint8) #reverse the colors\n",
    "        coords = cv2.findNonZero(gray) # Find all non-zero points (text)\n",
    "        x, y, w, h = cv2.boundingRect(coords) # Find minimum spanning bounding box\n",
    "        rect = image[y:y+h, x:x+w]\n",
    "        return rect\n",
    "\n",
    "    cropped_dataset = copy.deepcopy(dataset)\n",
    "    for idx in range(len(cropped_dataset)):\n",
    "        cropped_dataset[idx][\"image\"] = crop_excess_whitespace(remove_small_blobs(dataset[idx][\"image\"]))\n",
    "    return cropped_dataset\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    erode(percentage=0.1), # causes the notation to be thicker\n",
    "    dilate(percentage=0.1), # causes the notation to be thinner\n",
    "    transforms.ToTensor(), # convert from uint8 to [0, 1]\n",
    "    shrink(is_random=True),\n",
    "    paste_to_square(is_random=False),\n",
    "    salt_and_pepper(percentage=0.8), # introduces salt-and-pepper-noise\n",
    "    lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "    transforms.RandomRotation(degrees=(-8, 8)), # randomly rotate between -12 and 12 degrees\n",
    "    #lambda img: transforms.functional.invert(img), # inverts image to original color scheme\n",
    "    normalize(), # normalize mean and variance\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    shrink(is_random=False),\n",
    "    paste_to_square(is_random=False),\n",
    "    lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "    normalize(), # normalize mean and variance\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    shrink(is_random=False),\n",
    "    paste_to_square(is_random=False),\n",
    "    lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "    normalize(), # normalize mean and variance\n",
    "])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationEditions:  # This parameter chooses which edition is used for validation. The other editions are for training.\n",
    "    NONE: str = \"none\"\n",
    "    LU: str = \"lu\"\n",
    "    ZHANG: str = \"zhang\"\n",
    "    SIKU: str = \"siku\"\n",
    "    ZHU: str = \"zhu\"\n",
    "\n",
    "@dataclass\n",
    "class LabelType:\n",
    "    #PITCH: str = \"pitch\"\n",
    "    PITCH_BALANCED: str = \"pitch_balanced\"\n",
    "    #SECONDARY: str = \"secondary\"\n",
    "    SECONDARY_BALANCED: str = \"secondary_balanced\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, label_type, dataset: list, transform=None):\n",
    "        self.X = [entry[\"image\"] for entry in dataset]\n",
    "\n",
    "        if label_type.split(\"_\")[0] == \"pitch\":\n",
    "            self.y = [entry[\"annotation\"][\"pitch\"] for entry in dataset]\n",
    "        else:\n",
    "            self.y = [entry[\"annotation\"][\"secondary\"] for entry in dataset]\n",
    "\n",
    "        self.is_simple = [entry[\"is_simple\"] for entry in dataset]\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        label = self.y[idx]\n",
    "        is_simple = self.is_simple[idx]\n",
    "        return sample, label, is_simple\n",
    "            \n",
    "def get_dataloaders(dataset, validation_edition=ValidationEditions.NONE, label_type=LabelType.PITCH_BALANCED, is_test_loader=False):\n",
    "    def get_datasets():\n",
    "        if validation_edition is not ValidationEditions.NONE:\n",
    "            train_data = Dataset(label_type, [entry for entry in dataset if entry[\"edition\"]!=validation_edition], transform=train_transforms)\n",
    "            validation_data = Dataset(label_type, [entry for entry in dataset if entry[\"edition\"]==validation_edition], transform=validation_transforms)\n",
    "            test_data = Dataset(label_type, [entry for entry in dataset if entry[\"edition\"]==validation_edition], transform=test_transforms)\n",
    "        else:\n",
    "            # random shuffling the data\n",
    "            indices = [i for i in range(len(dataset))]\n",
    "            np.random.shuffle(indices)\n",
    "            split = int(len(dataset) * 0.8) # 80% of data\n",
    "            train_data = Dataset(label_type, dataset[:split], transform=train_transforms)\n",
    "            validation_data = Dataset(label_type, dataset[split:], transform=validation_transforms)\n",
    "            test_data = Dataset(label_type, dataset[split:], transform=test_transforms)\n",
    "        return train_data, validation_data, test_data\n",
    "    \n",
    "    def get_dataloaders(train_data, validation_data, test_data):\n",
    "        def get_sampler(y):\n",
    "            inverse_class_weights = 1/np.unique(y, return_counts=True)[1]\n",
    "            inverse_weigths = [inverse_class_weights[int(label)] for label in y]\n",
    "                \n",
    "            return torch.utils.data.WeightedRandomSampler(weights=inverse_weigths, num_samples=60000, replacement=True)\n",
    "\n",
    "        def get_val_sampler(y):\n",
    "            inverse_class_weights = 1/np.unique(y, return_counts=True)[1]\n",
    "            inverse_weigths = [inverse_class_weights[int(label)] for label in y]\n",
    "                \n",
    "            return torch.utils.data.WeightedRandomSampler(weights=inverse_weigths, num_samples=len(y), replacement=True)\n",
    "        \n",
    "        sample = \"balanced\" in label_type\n",
    "                    \n",
    "        train_sampler = get_sampler(train_data.y) if sample else None\n",
    "        validation_sampler = get_val_sampler(validation_data.y) if sample else None\n",
    "\n",
    "        loaders = {\n",
    "            'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                                  batch_size=100, \n",
    "                                                  sampler=train_sampler),\n",
    "\n",
    "            'validation'  : torch.utils.data.DataLoader(validation_data, \n",
    "                                                  batch_size=100, \n",
    "                                                  sampler=validation_sampler),\n",
    "            'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                                  batch_size=100),\n",
    "        }\n",
    "        \n",
    "        return loaders\n",
    "    \n",
    "    train_data, validation_data, test_data = get_datasets()\n",
    "    \n",
    "\n",
    "    return get_dataloaders(train_data, validation_data, test_data)\n",
    "\n",
    "def get_all_dataloaders(dataset):\n",
    "    dataloader_dict = {}\n",
    "    for val_edition in dataclasses.astuple(ValidationEditions()):\n",
    "        dataloader_dict[val_edition] = {}\n",
    "        for label_type in dataclasses.astuple(LabelType()):\n",
    "            dataloader_dict[val_edition][label_type] = get_dataloaders(dataset, val_edition, label_type)\n",
    "    return dataloader_dict\n",
    "\n",
    "def get_test_dataloaders(dataset):\n",
    "    return {\n",
    "        LabelType.PITCH_BALANCED: torch.utils.data.DataLoader(Dataset(LabelType.PITCH_BALANCED, dataset, transform=test_transforms), batch_size=100),\n",
    "        LabelType.SECONDARY_BALANCED: torch.utils.data.DataLoader(Dataset(LabelType.SECONDARY_BALANCED, dataset, transform=test_transforms), batch_size=100)\n",
    "    }\n",
    "\n",
    "def visualize_dataloader(dl, to_string=None):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    cols, rows = 5, 5\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(dl.dataset), size=(1,)).item()\n",
    "        img, label, _ = dl.dataset[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        \n",
    "        if to_string is not None:\n",
    "            label = class_to_annotation[to_string][label]\n",
    "            \n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bc87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(is_random=True):\n",
    "    def inner(input_image):\n",
    "         \n",
    "        t_size = random.randint(15, 22) if is_random else 20\n",
    "        \n",
    "        original_width = input_image.shape[-1]\n",
    "        original_height = input_image.shape[-2]\n",
    "        aspect_ratio = original_width / original_height * random.uniform(0.6, 1.5) if is_random else original_width / original_height\n",
    "        \n",
    "        if aspect_ratio > 1:\n",
    "            w = int(t_size)\n",
    "            h = int(t_size / aspect_ratio)\n",
    "        else: \n",
    "            w = int(t_size * aspect_ratio)\n",
    "            h = int(t_size)\n",
    "\n",
    "        output_image = transforms.Resize(size=(h, w), interpolation=transforms.InterpolationMode.NEAREST_EXACT)(input_image)\n",
    "        return output_image\n",
    "    \n",
    "    return inner\n",
    "\n",
    "def paste_to_square(is_random=True, target_size=28):\n",
    "    def inner(input_image):\n",
    "        ## Modify the function to extend the\n",
    "        ## input image to a square of 40x40.\n",
    "        ## Tip: This can be done by clever use\n",
    "        ## of the Pad function\n",
    "        ## https://pytorch.org/vision/stable/generated/torchvision.transforms.Pad.html\n",
    "        ## Also make sure that the added padding\n",
    "        ## on each side is random, i.e., the \n",
    "        ## data itself is augmented by its\n",
    "        ## position in the square.\n",
    "                \n",
    "        pad_width = target_size - input_image.shape[-1]\n",
    "        pad_height = target_size - input_image.shape[-2]\n",
    "        \n",
    "        left_pad = random.randint(0, pad_width) if is_random else pad_width//2\n",
    "        top_pad = random.randint(0, pad_height) if is_random else pad_height//2\n",
    "        \n",
    "        right_pad = pad_width - left_pad\n",
    "        bottom_pad = pad_height - top_pad\n",
    "            \n",
    "        output_image = transforms.Pad(padding=(left_pad, top_pad, right_pad, bottom_pad), fill=1)(input_image) \n",
    "        return output_image\n",
    "    \n",
    "    return inner\n",
    "\n",
    "def salt_and_pepper(percentage=0.1):\n",
    "    def inner(input_image):\n",
    "        output_image = input_image.numpy().squeeze()\n",
    "        if random.uniform(0, 1) < percentage/2:\n",
    "            amount = 0.02\n",
    "            output_image = random_noise(output_image, mode='salt', amount=amount)\n",
    "        if random.uniform(0, 1) < percentage/2:\n",
    "            amount = 0.01\n",
    "            output_image = random_noise(output_image, mode='pepper', amount=amount)\n",
    "        return torch.Tensor(output_image).unsqueeze(0)\n",
    "    return inner\n",
    "\n",
    "def erode(percentage=0.1):\n",
    "    def inner(input_image):\n",
    "        if random.uniform(0, 1) < percentage:  # only apply transformation according to percentage\n",
    "            kernel = np.ones((2,2),np.uint8)\n",
    "            output_image = cv2.erode(input_image, kernel, iterations=1)\n",
    "            return output_image\n",
    "        else:\n",
    "            return input_image\n",
    "    return inner\n",
    "\n",
    "def dilate(percentage=0.1):\n",
    "    def inner(input_image):\n",
    "        if random.uniform(0, 1) < percentage:  # only apply transformation according to percentage\n",
    "            kernel = np.ones((2,2),np.uint8)\n",
    "            output_image = cv2.dilate(input_image, kernel, iterations=1)\n",
    "            return output_image\n",
    "        else:\n",
    "            return input_image\n",
    "    return inner\n",
    "\n",
    "## Normalizes the dataset to have mean 0 and standard deviation of 1\n",
    "## --> good for later model performance\n",
    "#mean_list = torch.Tensor([torch.Tensor(entry[\"image\"]).mean() for entry in dataset])/255\n",
    "#mean = mean_list.mean()\n",
    "#std = mean_list.std()\n",
    "\n",
    "def normalize():\n",
    "    return transforms.Normalize(mean=0.7102, std=0.0914)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1414bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pitch_balanced', 'secondary_balanced'])\n"
     ]
    }
   ],
   "source": [
    "with open('train_data.pkl', 'rb') as handle:\n",
    "    train_dataset = pickle.load(handle)\n",
    "with open('test_data.pkl', 'rb') as handle:\n",
    "    test_dataset = pickle.load(handle)\n",
    "train_dataloaders = get_all_dataloaders(train_dataset)\n",
    "test_dataloaders = get_test_dataloaders(test_dataset)\n",
    "\n",
    "print(test_dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "723c79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_annotation =  {\n",
    "            \"pitch\": {0: \"HE\", 1: \"SI\", 2: \"YI\", 3: \"SHANG\", 4: \"GOU\", 5: \"CHE\", 6: \"GONG\", 7: \"FAN\", 8: \"LIU\", 9: \"WU\", 10: \"GAO_WU\"},\n",
    "            \"secondary\": {0: None, 1: \"DA_DUN\", 2: \"XIAO_ZHU\", 3: \"DING_ZHU\", 4: \"DA_ZHU\", 5: \"ZHE\", 6: \"YE\"}\n",
    "}\n",
    "\n",
    "annotation_to_class = {\n",
    "    \"pitch\": {\"HE\": 0, \"SI\": 1, \"YI\": 2, \"SHANG\": 3, \"GOU\": 4, \"CHE\": 5, \"GONG\": 6, \"FAN\": 7, \"LIU\": 8, \"WU\": 9, \"GAO_WU\": 10},\n",
    "    \"secondary\": {None: 0, \"DA_DUN\": 1, \"XIAO_ZHU\": 2, \"DING_ZHU\": 3, \"DA_ZHU\": 4, \"ZHE\": 5, \"YE\": 6}\n",
    "}\n",
    "\n",
    "## this function takes the path_to_folder (i.e., the folder where the dataset.json is in)\n",
    "## and returns a list of the dataset entries. Each entry consists of the keys:\n",
    "##     \"image_path\": \n",
    "##     \"type\": The type of the box (in our case, this is 'Music' only)\n",
    "##     \"annotation\": The annotation string\n",
    "##     \"image\": The image as uint8 array representation\n",
    "##     \"is_simple\": This is True if the notation is \"simple notation\" as opposed to \"composite notation\"\n",
    "def open_dataset(path_to_folder):\n",
    "    path_to_json = os.path.join(path_to_folder, \"dataset.json\")\n",
    "    with open(path_to_json) as file:\n",
    "        dataset_json = json.load(file)\n",
    "        output_list = []\n",
    "        \n",
    "        for idx in range(len(dataset_json)):\n",
    "            if dataset_json[idx][\"annotation\"][\"pitch\"] is None or dataset_json[idx][\"annotation\"][\"pitch\"] == \"None\":\n",
    "                continue\n",
    "            temp_dict = {}\n",
    "            temp_dict[\"image\"] = cv2.imread(os.path.join(path_to_folder, dataset_json[idx][\"image_path\"]), cv2.IMREAD_GRAYSCALE)\n",
    "            temp_dict[\"is_simple\"] = True if dataset_json[idx][\"annotation\"][\"secondary\"] == None else False\n",
    "            temp_dict[\"annotation\"] = {}\n",
    "            temp_dict[\"annotation\"][\"pitch\"] = annotation_to_class[\"pitch\"][dataset_json[idx][\"annotation\"][\"pitch\"]]\n",
    "            temp_dict[\"annotation\"][\"secondary\"] = annotation_to_class[\"secondary\"][dataset_json[idx][\"annotation\"][\"secondary\"]]\n",
    "            temp_dict[\"edition\"] = os.path.basename(dataset_json[idx][\"image_path\"]).split(\"_\")[0]\n",
    "            temp_dict[\"image_id\"] = \"_\".join(dataset_json[idx][\"image_path\"].split(\"_\")[:-1])\n",
    "            temp_dict[\"image_path\"] = os.path.basename(dataset_json[idx][\"image_path\"])\n",
    "            output_list.append(temp_dict)\n",
    "    return output_list\n",
    "\n",
    "def get_cropped_dataset(dataset):  ## creates a new dataset where each of the images is cropped\n",
    "    \n",
    "    def remove_small_blobs(image):  # remove small isolated connected black areas\n",
    "        noise_removal_threshold = 1  \n",
    "        mask = np.ones_like(image)*255\n",
    "        contours, hierarchy = cv2.findContours(255-image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        for contour in contours:\n",
    "          area = cv2.contourArea(contour)\n",
    "          if area >= noise_removal_threshold:\n",
    "            cv2.fillPoly(mask, [contour], 0)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def crop_excess_whitespace(image):\n",
    "        gray = 255*(image < 128).astype(np.uint8) #reverse the colors\n",
    "        coords = cv2.findNonZero(gray) # Find all non-zero points (text)\n",
    "        x, y, w, h = cv2.boundingRect(coords) # Find minimum spanning bounding box\n",
    "        rect = image[y:y+h, x:x+w]\n",
    "        return rect\n",
    "\n",
    "    cropped_dataset = copy.deepcopy(dataset)\n",
    "    for idx in range(len(cropped_dataset)):\n",
    "        cropped_dataset[idx][\"image\"] = crop_excess_whitespace(remove_small_blobs(dataset[idx][\"image\"]))\n",
    "    return cropped_dataset\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    erode(percentage=0.1), # causes the notation to be thicker\n",
    "    dilate(percentage=0.1), # causes the notation to be thinner\n",
    "    transforms.ToTensor(), # convert from uint8 to [0, 1]\n",
    "    shrink(is_random=True),\n",
    "    paste_to_square(is_random=False),\n",
    "    salt_and_pepper(percentage=0.8), # introduces salt-and-pepper-noise\n",
    "    lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "    transforms.RandomRotation(degrees=(-8, 8)), # randomly rotate between -12 and 12 degrees\n",
    "    #lambda img: transforms.functional.invert(img), # inverts image to original color scheme\n",
    "    normalize(), # normalize mean and variance\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    shrink(is_random=False),\n",
    "    paste_to_square(is_random=False),\n",
    "    lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "    normalize(), # normalize mean and variance\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    shrink(is_random=False),\n",
    "    paste_to_square(is_random=False),\n",
    "    lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "    normalize(), # normalize mean and variance\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a208889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/.local/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "/home/tristan/.local/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "/home/tristan/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n",
      "/home/tristan/.local/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "def get_transformation(model, l):\n",
    "    return model.get_representation(l)\n",
    "    \n",
    "def get_datasets(dataset, transformation=True):\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, return_pitch, dataset: list, model=None, tf=None):\n",
    "            model.eval()\n",
    "            concatenation = torch.cat([tf(entry[\"image\"]).unsqueeze(0) for entry in dataset])\n",
    "            \n",
    "            if tf:\n",
    "                self.X = get_transformation(model, concatenation)\n",
    "            else:\n",
    "                self.X = [tf(entry[\"image\"]) for entry in dataset]\n",
    "                \n",
    "            self.original = [entry[\"image\"] for entry in dataset]\n",
    "            \n",
    "            if return_pitch:\n",
    "                self.y = [entry[\"annotation\"][\"pitch\"] for entry in dataset]\n",
    "            else:\n",
    "                self.y = [entry[\"annotation\"][\"secondary\"] for entry in dataset]\n",
    "\n",
    "\n",
    "            self.transform = tf\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sample = self.X[idx]\n",
    "            label = self.y[idx]\n",
    "            return sample, label\n",
    "    \n",
    "    pitch_data = Dataset(return_pitch=True, dataset=dataset, model=pitch_model, tf=validation_transforms)\n",
    "    secondary_data = Dataset(return_pitch=False, dataset=dataset, model=secondary_model, tf=validation_transforms)\n",
    "    return {\"pitch\": pitch_data, \"secondary\": secondary_data}\n",
    "\n",
    "with open(\"train_data.pkl\", \"rb\") as file_handle:\n",
    "    dataset = pickle.load(file_handle)\n",
    "datasets = get_datasets(dataset, False)\n",
    "\n",
    "import umap\n",
    "\n",
    "datasets = get_datasets(dataset, True)\n",
    "umap_pitch = umap.UMAP(random_state=42).fit(datasets[\"pitch\"].X.detach().numpy())\n",
    "umap_secondary = umap.UMAP(random_state=42).fit(datasets[\"secondary\"].X.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8410524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"pitch\"][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cd3141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method UMAP.transform of UMAP(random_state=42, tqdm_kwds={'bar_format': '{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]', 'desc': 'Epochs completed', 'disable': True})>\n"
     ]
    }
   ],
   "source": [
    "print(umap_pitch.transform)\n",
    "import pickle\n",
    "with open(\"umap_models.pkl\", \"ab\") as file_handle:\n",
    "    pickle.dump({\"pitch\": umap_pitch.transform, \"secondary\": umap_secondary.transform}, file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16ba1bd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_transformations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24389/3201484334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"editions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edition\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m\"display_images\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisplay_transformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"pitch_embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_pitch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"secondary_embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_secondary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24389/3201484334.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"editions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edition\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m\"display_images\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisplay_transformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"pitch_embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_pitch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"secondary_embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_secondary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_transformations' is not defined"
     ]
    }
   ],
   "source": [
    "umap_embedding = {\n",
    "    \"editions\": [entry[\"edition\"] for entry in dataset],\n",
    "    \"annotations\": [entry[\"annotation\"] for entry in dataset],\n",
    "    \"display_images\": torch.cat([display_transformations(entry[\"image\"]).unsqueeze(0) for entry in dataset]),\n",
    "    \"pitch_embeddings\": torch.Tensor(umap_pitch.embedding_),\n",
    "    \"secondary_embeddings\": torch.Tensor(umap_secondary.embedding_)\n",
    "}\n",
    "with open(\"umap_embeddings.pkl\", \"ab\") as file_handle:\n",
    "    pickle.dump(umap_embedding, file_handle)\n",
    "    \n",
    "plt.imshow(umap_embedding[\"display_images\"][0].squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4de67eb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'umap_embeddings.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24389/2580782472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"umap_embeddings.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pitch_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mumap_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pitch_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pitch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Spectral'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pitch Embeddings (UMAP)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'umap_embeddings.pkl'"
     ]
    }
   ],
   "source": [
    "with open(\"umap_embeddings.pkl\", \"rb\") as file_handle:\n",
    "    umap_embeddings = pickle.load(file_handle)\n",
    "\n",
    "plt.scatter(umap_embeddings[\"pitch_embeddings\"][:, 0], umap_embeddings[\"pitch_embeddings\"][:, 1], s=5, c=datasets[\"pitch\"].y, cmap='Spectral')\n",
    "plt.title('Pitch Embeddings (UMAP)', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbedee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_embeddings[\"secondary_embeddings\"][:, 0], umap_embeddings[\"secondary_embeddings\"][:, 1], s=5, c=datasets[\"secondary\"].y, cmap='Spectral')\n",
    "plt.title('Secondary Embeddings (UMAP)', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834050eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "\n",
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self, num_output_classes):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=num_output_classes)\n",
    "        self.logits = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.logits(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_representation(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class TemperatureScalingCalibrationModule(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # the single temperature scaling parameter, the initialization value doesn't\n",
    "        # seem to matter that much based on some ad-hoc experimentation\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def get_representation(self, x):\n",
    "        return self.model.get_representation(x)\n",
    "        \n",
    "    def forward_unscaled(self, x):\n",
    "        logits = self.model(x)\n",
    "        scores = nn.functional.softmax(logits, dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def forward(self, x):\n",
    "        scaled_logits = self.forward_logit(x)\n",
    "        scores = nn.functional.softmax(scaled_logits, dim=1)\n",
    "        return scores\n",
    "\n",
    "    def forward_logit(self, x):      \n",
    "        logits = self.model(x)\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def fit(self, data_loader, n_epochs: int = 10, batch_size: int = 64, lr: float = 0.01):\n",
    "        \"\"\"fits the temperature scaling parameter.\"\"\"\n",
    "        assert isinstance(data_loader, DataLoader), \"data_loader must be an instance of DataLoader\"\n",
    "    \n",
    "        print(self.temperature.requires_grad)\n",
    "        \n",
    "        self.freeze_base_model()\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in data_loader:\n",
    "                images, labels, _ = batch  \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "                self.zero_grad()\n",
    "                scaled_logits = self.forward_logit(images)  # Use forward to get scaled logits\n",
    "                loss = criterion(scaled_logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "        return self\n",
    "\n",
    "\n",
    "    def freeze_base_model(self):\n",
    "        \"\"\"remember to freeze base model's parameters when training temperature scaler\"\"\"\n",
    "        self.model.eval()\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        return self\n",
    "\n",
    "def get_transforms():\n",
    "    def remove_small_blobs(image):  # remove small isolated connected black areas\n",
    "        noise_removal_threshold = 1  \n",
    "        mask = np.ones_like(image)*255\n",
    "        contours, hierarchy = cv2.findContours(255-image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        for contour in contours:\n",
    "          area = cv2.contourArea(contour)\n",
    "          if area >= noise_removal_threshold:\n",
    "            cv2.fillPoly(mask, [contour], 0)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def crop_excess_whitespace(image):\n",
    "        gray = 255*(image < 128).astype(np.uint8) #reverse the colors\n",
    "        coords = cv2.findNonZero(gray) # Find all non-zero points (text)\n",
    "        x, y, w, h = cv2.boundingRect(coords) # Find minimum spanning bounding box\n",
    "        rect = image[y:y+h, x:x+w]\n",
    "        return rect\n",
    "\n",
    "\n",
    "    def shrink(target_size=28):\n",
    "        def inner(input_image):\n",
    "            t_size = target_size - 7\n",
    "\n",
    "            original_width = input_image.shape[-1]\n",
    "            original_height = input_image.shape[-2]\n",
    "            aspect_ratio = original_width / original_height\n",
    "\n",
    "            if aspect_ratio > 1:\n",
    "                w = int(t_size)\n",
    "                h = int(t_size / aspect_ratio)\n",
    "            else: \n",
    "                w = int(t_size * aspect_ratio)\n",
    "                h = int(t_size)\n",
    "\n",
    "            output_image = transforms.Resize(size=(h, w), interpolation=transforms.InterpolationMode.NEAREST_EXACT)(input_image)\n",
    "            return output_image\n",
    "\n",
    "        return inner\n",
    "\n",
    "    def paste_to_square(target_size=28):\n",
    "        def inner(input_image):\n",
    "            ## Modify the function to extend the\n",
    "            ## input image to a square of 40x40.\n",
    "            ## Tip: This can be done by clever use\n",
    "            ## of the Pad function\n",
    "            ## https://pytorch.org/vision/stable/generated/torchvision.transforms.Pad.html\n",
    "            ## Also make sure that the added padding\n",
    "            ## on each side is random, i.e., the \n",
    "            ## data itself is augmented by its\n",
    "            ## position in the square.\n",
    "\n",
    "            pad_width = target_size - input_image.shape[-1]\n",
    "            pad_height = target_size - input_image.shape[-2]\n",
    "\n",
    "            left_pad = pad_width//2\n",
    "            top_pad = pad_height//2\n",
    "\n",
    "            right_pad = pad_width - left_pad\n",
    "            bottom_pad = pad_height - top_pad\n",
    "\n",
    "            output_image = transforms.Pad(padding=(left_pad, top_pad, right_pad, bottom_pad), fill=1)(input_image) \n",
    "            return output_image\n",
    "\n",
    "        return inner\n",
    "\n",
    "    def normalize():\n",
    "        mean = 0.8497\n",
    "        std = 0.0518\n",
    "        return transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    evaluation_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        shrink(),\n",
    "        paste_to_square(),\n",
    "        lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "        normalize(), # normalize mean and variance\n",
    "    ])\n",
    "\n",
    "    display_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        shrink(target_size=60),\n",
    "        paste_to_square(target_size=60),\n",
    "        lambda img: transforms.functional.invert(img), # inverts image, needed for rotations\n",
    "        normalize(), # normalize mean and variance\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"evaluation\": evaluation_transforms,\n",
    "        \"display\": display_transforms,\n",
    "        \"class_to_annotation\": {\n",
    "            \"pitch\": {0: \"HE\", 1: \"SI\", 2: \"YI\", 3: \"SHANG\", 4: \"GOU\", 5: \"CHE\", 6: \"GONG\", 7: \"FAN\", 8: \"LIU\", 9: \"WU\", 10: \"GAO_WU\"},\n",
    "            \"secondary\": {0: None, 1: \"DA_DUN\", 2: \"XIAO_ZHU\", 3: \"DING_ZHU\", 4: \"DA_ZHU\", 5: \"ZHE\", 6: \"YE\"}\n",
    "        }}\n",
    "\n",
    "def load_model():\n",
    "    pitch_model = TemperatureScalingCalibrationModule(FashionCNN(num_output_classes=11))\n",
    "    pitch_model.load_state_dict(torch.load(\"./model_pitch.std\"))\n",
    "    secondary_model = TemperatureScalingCalibrationModule(FashionCNN(num_output_classes=7))\n",
    "    secondary_model.load_state_dict(torch.load(\"./model_secondary.std\"))\n",
    "    with open(\"umap_models.pkl\", \"rb\") as file_handle:\n",
    "        umap_models = pickle.load(file_handle)\n",
    "    with open(\"umap_embeddings.pkl\", \"rb\") as file_handle:\n",
    "        umap_embeddings = pickle.load(file_handle)\n",
    "    \n",
    "    return {\n",
    "        \"model\": {\"pitch\": pitch_model, \"secondary\": secondary_model},\n",
    "        \"umap_models\": umap_models,\n",
    "        \"umap_embeddings\": umap_embeddings,\n",
    "    }\n",
    "\n",
    "def predict_similar(image_list, models, transforms):\n",
    "    concatenation = torch.cat([transforms[\"evaluation\"](entry).unsqueeze(0) for entry in image_list])\n",
    "    \n",
    "    pitch_latent = pitch_model.get_representation(concatenation).detach().numpy()\n",
    "    secondary_latent = secondary_model.get_representation(concatenation).detach().numpy()\n",
    "\n",
    "    current_pitch_embedding = models[\"umap_models\"][\"pitch\"](pitch_latent)\n",
    "    current_secondary_embedding = models[\"umap_models\"][\"secondary\"](secondary_latent)\n",
    "\n",
    "    pitch_nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(models[\"umap_embeddings\"][\"pitch_embeddings\"])\n",
    "    pitch_distances, pitch_indices = pitch_nbrs.kneighbors(current_pitch_embedding)\n",
    "\n",
    "    secondary_nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(models[\"umap_embeddings\"][\"secondary_embeddings\"])\n",
    "    secondary_distances, secondary_indices = secondary_nbrs.kneighbors(current_secondary_embedding)\n",
    "\n",
    "    def get_dict_pitch(distance, idx):\n",
    "        image = models[\"umap_embeddings\"][\"display_images\"][idx]\n",
    "        edition = models[\"umap_embeddings\"][\"editions\"][idx]\n",
    "        annotation = models[\"umap_embeddings\"][\"annotations\"][idx][\"pitch\"]\n",
    "        return {\"image\": image,\n",
    "                \"edition\": edition,\n",
    "                \"distance\": distance,\n",
    "                \"similarity\": 1/distance if distance > 0 else 9999,\n",
    "                \"annotation\": transforms[\"class_to_annotation\"][\"pitch\"][annotation]}\n",
    "\n",
    "    def get_dict_secondary(distance, idx):\n",
    "        image = models[\"umap_embeddings\"][\"display_images\"][idx]\n",
    "        edition = models[\"umap_embeddings\"][\"editions\"][idx]\n",
    "        annotation = models[\"umap_embeddings\"][\"annotations\"][idx][\"secondary\"]\n",
    "        return {\"image\": image,\n",
    "                \"edition\": edition,\n",
    "                \"distance\": distance,\n",
    "                \"similarity\": 1/distance if distance > 0 else 9999,\n",
    "                \"annotation\": transforms[\"class_to_annotation\"][\"secondary\"][annotation]}\n",
    "\n",
    "    output = {\"pitch\": \n",
    "    [[get_dict_pitch(distance, idx) for distance, idx in zip(pitch_distances[IDX], pitch_indices[IDX])] for IDX in range(len(image_list))],\n",
    "            \"secondary\": \n",
    "    [[get_dict_secondary(distance, idx) for distance, idx in zip(secondary_distances[IDX], secondary_indices[IDX])] for IDX in range(len(image_list))]\n",
    "           }\n",
    "\n",
    "    return output\n",
    "\n",
    "def predict_notation(image_list, models, transforms):\n",
    "    concatenation = torch.cat([transforms[\"evaluation\"](entry).unsqueeze(0) for entry in image_list])\n",
    "    \n",
    "    pitch_predictions = pitch_model.forward(concatenation).detach().numpy()\n",
    "    secondary_predictions = secondary_model.forward(concatenation).detach().numpy()\n",
    "    \n",
    "    def get_first_three(tensor):\n",
    "        first_confidence = tensor.max()\n",
    "        first_label = tensor.argmax()\n",
    "        tensor[first_label] = 0\n",
    "            \n",
    "        second_confidence = tensor.max()\n",
    "        second_label = tensor.argmax()\n",
    "        tensor[second_label] = 0\n",
    "            \n",
    "        third_confidence = tensor.max()\n",
    "        third_label = tensor.argmax()\n",
    "        tensor[third_label] = 0\n",
    "        \n",
    "        return [first_label, second_label, third_label], [first_confidence, second_confidence, third_confidence]\n",
    "            \n",
    "\n",
    "    def get_dict_pitch(idx):\n",
    "        annotations, confidences = get_first_three(pitch_predictions[idx])\n",
    "        annotations = [transformations[\"class_to_annotation\"][\"pitch\"][annotation] for annotation in annotations]\n",
    "        return {\n",
    "            \"annotations\": annotations,\n",
    "            \"confidences\": confidences\n",
    "        }\n",
    "\n",
    "    def get_dict_secondary(idx):\n",
    "        annotations, confidences = get_first_three(secondary_predictions[idx])\n",
    "        annotations = [transformations[\"class_to_annotation\"][\"secondary\"][annotation] for annotation in annotations]\n",
    "        return {\n",
    "            \"annotations\": annotations,\n",
    "            \"confidences\": confidences\n",
    "        }\n",
    "\n",
    "    output = {\"pitch\": \n",
    "    [get_dict_pitch(idx) for idx in range(len(image_list))],\n",
    "            \"secondary\": \n",
    "    [get_dict_secondary(idx) for idx in range(len(image_list))]\n",
    "           }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c025f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "186d6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "transformations = get_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0ecb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, images_, labels_):  \n",
    "    def loss_function(x, y):\n",
    "        return nn.NLLLoss()(x, y)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_step = len(images_)\n",
    "    \n",
    "    print(f\"Validating with\")\n",
    "    \n",
    "    pred_y = []\n",
    "    full_labels = []\n",
    "    outputs = []\n",
    "        \n",
    "    for i, (images, labels) in enumerate(zip(images_, labels_)):\n",
    "        images = images.unsqueeze(0)\n",
    "        # gives batch data, normalize x when iterate train_loader\n",
    "        b_x = images   # batch x\n",
    "        b_y = labels\n",
    "\n",
    "        output = model(b_x)\n",
    "        outputs.append(output)\n",
    "        loss = loss_function(output[0], torch.tensor(b_y))\n",
    "        pred_y += torch.argmax(output, dim=1)\n",
    "        full_labels += [labels]\n",
    "    \n",
    "    labels = torch.Tensor(full_labels)\n",
    "    pred_y = torch.Tensor(pred_y)\n",
    "    \n",
    "    total_accuracy = (pred_y == labels).sum() / float(len(pred_y))\n",
    "\n",
    "    print ('    Acc.(Total): {:.2f}%%'.format(total_accuracy*100))\n",
    "    \n",
    "    return outputs, pred_y\n",
    "\n",
    "def get_transformation(model, l):\n",
    "    return model.get_representation(l)\n",
    "    \n",
    "def get_datasets(dataset, transformation=True):\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, return_pitch, dataset: list, model=None, transform=None):\n",
    "            model.eval()\n",
    "            concatenation = torch.cat([transform(entry[\"image\"]).unsqueeze(0) for entry in dataset])\n",
    "            \n",
    "            if transformation:\n",
    "                self.X = get_transformation(model, concatenation)\n",
    "            else:\n",
    "                self.X = [transform(entry[\"image\"]) for entry in dataset]\n",
    "                \n",
    "            self.original = [entry[\"image\"] for entry in dataset]\n",
    "            \n",
    "            if return_pitch:\n",
    "                self.y = [entry[\"annotation\"][\"pitch\"] for entry in dataset]\n",
    "            else:\n",
    "                self.y = [entry[\"annotation\"][\"secondary\"] for entry in dataset]\n",
    "\n",
    "\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sample = self.X[idx]\n",
    "            label = self.y[idx]\n",
    "            return sample, label\n",
    "    \n",
    "    pitch_data = Dataset(return_pitch=True, dataset=dataset, model=model[\"model\"][\"pitch\"], transform=transformations[\"evaluation\"])\n",
    "    secondary_data = Dataset(return_pitch=False, dataset=dataset, model=model[\"model\"][\"secondary\"], transform=transformations[\"evaluation\"])\n",
    "    return {\"pitch\": pitch_data, \"secondary\": secondary_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21ce354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edition_data.pkl\", \"rb\") as file_handle:\n",
    "    dataset = pickle.load(file_handle)\n",
    "datasets = get_datasets(dataset, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32aa1c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating with\n",
      "L1 tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0121, 0.0121, 0.0121,  ..., 0.0121, 0.0121, 1.3222]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.6464, 1.6464, 1.6464,  ..., 1.6464, 1.6464, 1.6464],\n",
      "          [1.6464, 1.6464, 1.6464,  ..., 1.6464, 1.6464, 1.6464],\n",
      "          [1.6464, 1.6464, 1.6464,  ..., 1.6464, 1.6464, 1.6464],\n",
      "          ...,\n",
      "          [1.6464, 1.6464, 1.6464,  ..., 1.6464, 1.6464, 1.6464],\n",
      "          [1.6464, 1.6464, 1.6464,  ..., 1.6464, 1.6464, 1.6464],\n",
      "          [1.6464, 1.6464, 1.6464,  ..., 1.6464, 1.6464, 1.6464]],\n",
      "\n",
      "         [[1.5020, 1.5020, 1.5020,  ..., 1.5020, 1.5020, 2.8361],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "L2 tensor([[[[0.0000e+00, 3.8999e-01, 2.2601e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 1.7112e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 2.4546e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 2.1923e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.9939e-01, 8.5304e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 3.1185e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 2.9030e+00, 2.2701e-03, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 9.7900e-01, 2.0813e+00, 9.7939e-01,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6621e-01,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 8.4454e-01, 0.0000e+00, 1.3703e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.7134e+00, 4.1540e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 1.2845e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 1.1735e+00, 1.6886e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 7.0505e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.4305e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 6.8628e-01, 6.0581e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 7.9500e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 9.7391e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 1.2997e+00, 1.4201e+00, 3.6831e-02, 0.0000e+00,\n",
      "           0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "FC1 tensor([[ 1.0002e-01, -4.7431e-01,  1.3034e+00,  9.3591e-01, -4.3966e-01,\n",
      "          9.6014e-01,  1.1300e+00, -1.3947e+00,  1.3756e+00,  2.3882e-01,\n",
      "          6.2021e-01,  1.8789e+00,  4.4104e-01,  3.6444e-01,  7.4590e-01,\n",
      "          1.9458e+00,  2.9609e+00,  5.7992e-01,  1.4932e+00, -1.6877e+00,\n",
      "         -2.5622e-01, -1.7380e-01,  2.2690e-01,  8.9411e-01,  1.1601e+00,\n",
      "         -7.1766e-01, -9.4365e-01, -1.0295e+00,  9.4755e-01,  4.9637e-01,\n",
      "         -1.3148e+00, -8.4980e-01, -1.2156e+00,  1.0055e+00, -1.6876e+00,\n",
      "         -1.1901e+00,  2.5667e+00,  4.5957e-02, -1.4927e+00,  5.2983e-01,\n",
      "         -3.3887e-01, -7.4410e-01,  1.7966e+00, -7.6102e-01, -2.0941e+00,\n",
      "          1.2429e+00, -3.1130e-01, -5.3686e-01,  4.1512e-01, -5.5232e-01,\n",
      "          2.2077e-02,  8.8408e-01,  1.4148e-01, -9.7062e-01, -6.8591e-01,\n",
      "         -1.1103e+00, -7.8385e-01,  6.5535e-01,  1.3375e+00,  7.4355e-01,\n",
      "         -3.7348e-01, -2.4174e+00,  5.6603e-01,  2.0724e+00, -5.8077e-01,\n",
      "          1.4927e+00, -5.7339e-01, -8.3019e-01,  1.3943e+00, -1.6460e+00,\n",
      "         -9.8335e-01,  1.4417e+00, -1.6094e+00,  4.6921e-01,  1.4222e+00,\n",
      "         -3.6878e-01,  4.3587e-02, -2.1239e+00, -2.0841e+00,  1.3205e+00,\n",
      "          9.4182e-01, -4.7259e-02,  2.1484e+00,  6.9466e-02, -1.0260e+00,\n",
      "         -7.2133e-02, -1.4616e+00,  2.3289e-01, -4.0904e-01,  1.3604e+00,\n",
      "          2.4360e+00, -4.6778e-01, -6.2667e-01,  1.2705e+00,  4.9775e-01,\n",
      "          7.6921e-01,  1.7167e+00, -2.5487e-01,  1.2801e+00,  6.5883e-01,\n",
      "          2.8860e+00, -5.2826e-02, -3.7217e-01,  1.7376e+00, -1.0917e+00,\n",
      "          6.4643e-01, -3.3197e+00,  1.3408e+00, -2.0105e+00,  1.2447e+00,\n",
      "          2.6590e-01,  5.4746e-02,  1.2769e+00, -2.7831e-01,  8.8670e-01,\n",
      "         -4.0714e-02,  1.4104e+00,  1.8186e+00,  5.0382e-01,  2.8299e+00,\n",
      "         -2.4291e+00,  1.1184e+00, -3.5456e-01, -2.7767e-01, -2.3622e+00,\n",
      "          4.9676e-01,  2.1099e-01,  2.0426e-01,  1.6295e+00, -6.4512e-01,\n",
      "         -1.2551e+00, -9.6063e-01,  2.1903e+00,  7.2395e-01, -1.4641e-01,\n",
      "         -6.7873e-01,  1.6446e-01,  1.0659e+00,  7.3697e-01, -6.2793e-01,\n",
      "          3.3106e-02,  1.8589e+00,  5.4618e-01, -1.6191e+00,  9.0681e-02,\n",
      "          6.0222e-01,  2.7163e+00, -1.4305e+00,  3.3597e-01,  7.9925e-01,\n",
      "          1.1129e+00,  1.0807e+00,  3.0655e-01,  1.1966e+00, -4.1426e-01,\n",
      "          1.7608e+00,  1.0131e-01,  1.2857e+00,  1.4696e+00,  1.2418e+00,\n",
      "          3.4559e-01, -5.4997e-02, -4.9023e-02,  1.5001e+00, -2.2446e-01,\n",
      "          4.3983e-01, -7.4185e-01,  2.4437e+00,  2.0996e+00,  1.0336e+00,\n",
      "          5.1477e-01,  1.4291e-01,  7.4117e-02,  9.0352e-02,  1.2592e+00,\n",
      "          4.0857e-01, -6.7910e-01, -2.9229e-01, -2.2251e-01, -1.2963e+00,\n",
      "          1.1248e+00,  3.2614e-01,  5.3704e-01, -1.8425e+00,  4.4596e-01,\n",
      "          7.6771e-01,  1.4894e-01, -8.6215e-01, -5.8023e-01, -1.7636e-01,\n",
      "          1.7168e+00,  1.0975e+00, -2.4186e+00,  4.2293e-02, -7.3012e-02,\n",
      "          4.2730e-01, -5.3057e-01, -2.0889e+00, -9.3830e-01, -2.6986e+00,\n",
      "          1.7235e+00, -6.4907e-01, -1.5641e+00, -6.5280e-01,  2.7656e+00,\n",
      "          8.6306e-01, -1.1503e+00, -1.9040e+00, -3.2678e-01,  2.9017e-01,\n",
      "         -2.4877e-01,  1.2670e+00, -2.7440e+00, -6.0129e-01, -4.7672e-01,\n",
      "         -8.9631e-01,  1.0958e+00,  1.9856e-01,  5.9733e-01, -6.8283e-01,\n",
      "          2.4101e-01,  6.2503e-01,  6.6040e-01, -4.6105e-01, -1.0421e+00,\n",
      "          1.8831e+00, -1.0836e+00, -6.5194e-01, -4.9754e+00,  7.0192e-01,\n",
      "          2.2197e+00,  4.0353e-01, -7.0331e-01, -1.6193e+00,  9.8150e-01,\n",
      "         -8.2951e-01, -6.8136e-01,  3.8268e-01, -2.2684e+00,  1.2416e+00,\n",
      "          7.1700e-01, -1.4191e+00, -1.2685e+00, -3.8893e-01,  9.4018e-01,\n",
      "          7.2010e-01,  1.3656e+00, -1.7301e-01,  1.4952e+00, -1.8202e-01,\n",
      "         -1.2282e+00, -1.1171e+00, -1.7944e+00,  5.9459e-01,  1.4354e+00,\n",
      "         -7.0292e-01,  1.9605e+00,  8.5323e-01, -1.0149e+00, -1.4797e-01,\n",
      "          1.0988e+00,  1.4528e+00,  1.1607e-01, -3.7382e+00, -9.6685e-01,\n",
      "          1.1998e+00, -7.6023e-01, -1.1549e+00, -1.6544e+00, -1.7051e+00,\n",
      "         -2.1056e-01,  7.0721e-01,  1.4202e+00, -1.3265e+00, -9.9418e-01,\n",
      "          1.0635e+00,  1.9359e+00,  2.7667e-01, -1.4103e+00,  1.0419e+00,\n",
      "         -1.3691e+00, -4.0179e+00,  1.2095e+00, -1.2839e+00,  2.0312e-01,\n",
      "          8.0001e-02, -1.2880e+00, -1.5427e-01,  2.2031e-01,  2.7177e-01,\n",
      "          9.6733e-02,  2.7124e+00, -9.4069e-01, -2.5749e-02, -1.4358e+00,\n",
      "         -1.1376e+00,  2.7251e-01, -1.8737e+00, -1.3435e+00, -2.4935e+00,\n",
      "         -4.1044e-01, -1.1421e+00, -1.3298e+00,  5.0708e-01, -2.0614e-01,\n",
      "          2.6127e-01,  1.7299e-01, -1.4524e+00, -2.0651e+00,  5.8734e-01,\n",
      "         -1.9067e+00, -8.8510e-01,  8.4696e-01, -7.0530e-01,  9.9566e-01,\n",
      "          5.8319e-01,  1.2727e+00, -5.9494e-01, -4.3020e-01, -2.2121e-01,\n",
      "          6.9986e-01, -3.6014e+00, -1.6087e-01, -2.4158e-01, -8.2198e-01,\n",
      "          5.6499e-01, -4.2165e-01,  2.6361e+00, -1.6124e+00,  4.1037e-01,\n",
      "         -4.0703e-01,  1.0317e+00,  1.0894e+00,  6.7573e-01, -3.3970e-01,\n",
      "          1.2825e-01, -3.1328e-01,  5.8655e-02,  1.5532e+00, -1.6718e+00,\n",
      "          5.3937e-03, -4.8823e-01, -9.2949e-01, -1.6268e+00, -1.1891e+00,\n",
      "         -1.4968e-01,  1.5372e+00,  1.0985e+00, -1.1432e-01,  1.4435e+00,\n",
      "         -4.9371e-01, -6.6561e-01, -2.5821e-01, -4.1629e-02, -4.8526e-01,\n",
      "         -3.1667e-01, -5.3872e-01,  3.1979e-01, -3.0061e-01, -1.3819e-01,\n",
      "         -3.3022e+00,  2.7895e-01,  1.1056e+00,  1.6364e+00,  4.9778e-01,\n",
      "          8.9915e-01,  1.0370e+00,  3.9653e-01,  1.0873e+00, -3.8139e-01,\n",
      "         -1.7901e+00,  1.5914e+00, -1.9420e+00,  1.4790e+00, -1.0498e+00,\n",
      "          7.8429e-02,  7.7169e-01,  5.6160e-01,  1.6844e+00, -5.2367e-01,\n",
      "         -4.1710e-01, -8.2923e-01,  7.1246e-01,  2.8014e+00,  1.3619e-02,\n",
      "         -2.8537e-01, -3.9827e-01, -1.9385e+00, -8.5497e-01, -2.4701e+00,\n",
      "         -1.4472e+00, -2.2796e-01,  3.6944e-01, -1.8688e-01,  3.2476e-01,\n",
      "         -4.8853e-01, -6.5685e-01, -9.7899e-01, -3.3443e-01,  5.9578e-01,\n",
      "         -1.3618e+00,  9.7383e-01, -1.8812e+00,  4.9870e-01,  1.9378e-01,\n",
      "          1.7979e+00, -1.0420e+00, -1.2085e+00,  4.6313e-01, -3.4582e+00,\n",
      "          1.4068e+00,  6.1765e-01, -6.1012e-01, -1.4897e+00,  1.7073e+00,\n",
      "         -4.4802e-01,  1.6124e+00,  6.3350e-01,  1.4120e+00,  1.8667e+00,\n",
      "          1.1033e-01, -1.8135e+00,  3.9970e-01, -1.3637e+00, -1.6605e-01,\n",
      "         -8.9694e-01,  1.4541e+00,  3.7111e-01, -1.5135e+00, -2.3554e+00,\n",
      "          1.3034e+00, -2.3040e-02, -5.1088e-01, -1.7676e+00, -1.3172e-02,\n",
      "         -1.1428e-01,  1.4042e+00, -2.1056e-01, -4.7826e-01, -2.8841e-02,\n",
      "          1.2398e+00, -1.0525e+00, -8.8425e-01, -7.0374e-01,  1.8828e-01,\n",
      "         -4.6071e-01, -8.8774e-01,  1.5854e+00,  1.0050e+00, -9.7251e-01,\n",
      "         -9.1990e-01,  5.6174e-01,  1.7779e-02,  4.9557e-01,  2.5626e-01,\n",
      "          5.4604e-01,  3.9795e-01, -1.3056e+00,  5.1594e-01, -3.9810e+00,\n",
      "          1.0250e+00,  1.6253e+00,  4.1070e-01,  1.8381e-01, -6.9507e-01,\n",
      "          4.4906e-01, -2.4525e+00, -1.0560e-01, -3.7095e-01, -1.4739e+00,\n",
      "          3.0054e-01, -2.4240e-01,  1.2169e+00,  2.0215e-01,  3.4286e+00,\n",
      "          2.7183e-01, -7.0870e-01,  4.3483e-01,  9.9413e-01,  1.0673e+00,\n",
      "          1.4397e-01, -5.9237e-01, -1.5332e+00,  6.0910e-01, -2.7336e-01,\n",
      "          4.4519e-01, -1.4599e+00, -1.2508e+00, -5.0402e-01,  1.2379e+00,\n",
      "         -2.6363e+00,  1.2037e-01,  1.0248e+00,  2.4596e+00,  4.0896e-01,\n",
      "         -5.7273e-01,  1.0949e+00, -2.6602e+00, -2.8679e-01,  3.8672e+00,\n",
      "         -6.6970e-02, -9.1165e-01,  4.3441e-01,  5.6297e-01, -3.6812e-02,\n",
      "         -1.9658e+00,  9.9804e-01, -8.8297e-01,  1.1310e+00, -8.4363e-01,\n",
      "          1.6221e-01, -2.6120e+00,  6.3257e-01, -6.5977e-01,  4.0370e-01,\n",
      "         -2.2514e-01, -1.1418e+00,  1.1688e+00,  9.4193e-02, -6.9966e-01,\n",
      "         -2.1223e+00, -2.7752e-01, -1.8218e-01,  7.3100e-01,  1.0209e+00,\n",
      "         -1.9632e+00, -6.8421e-01, -1.3656e+00,  1.1861e+00,  2.0431e+00,\n",
      "          8.0709e-01,  1.8571e+00,  1.6527e+00,  1.8411e+00, -7.2737e-01,\n",
      "         -5.9375e-01, -1.1644e+00,  1.4185e+00, -5.4042e-01,  2.3193e-01,\n",
      "          1.0039e-02, -1.0386e-02, -1.3925e+00, -1.3730e+00, -1.3089e+00,\n",
      "          1.0262e+00,  9.4725e-02,  1.1330e-01, -1.4741e+00, -4.7381e-01,\n",
      "         -3.7306e-01,  3.2312e-01, -2.3595e-03,  3.2854e-01, -1.2281e+00,\n",
      "         -3.7252e+00,  1.6637e+00, -5.8282e-01, -8.6999e-01, -2.9385e-01,\n",
      "         -8.9411e-01,  2.1554e+00, -7.8597e-02,  8.4854e-01,  2.0603e-01,\n",
      "         -1.2437e+00,  5.3163e+00,  1.8561e+00, -2.9191e-01, -1.1367e+00,\n",
      "         -4.8156e-01,  1.7653e+00, -7.3093e-01,  8.2294e-01, -4.9028e-02,\n",
      "          9.0706e-01, -6.9615e-01, -1.2305e-01, -6.7713e-01, -4.3681e+00,\n",
      "         -1.8224e+00, -4.3820e-01,  1.0265e+00,  8.1065e-01,  3.0881e-01,\n",
      "         -6.4871e-01, -1.5954e+00,  7.5271e-01, -3.1122e+00, -2.9221e-01,\n",
      "          5.2751e-01,  2.2893e+00, -3.5682e+00, -2.4761e+00,  6.1138e-01,\n",
      "          7.5139e-01, -2.0858e+00,  2.0365e+00, -5.5274e-01, -6.1998e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "FC2 tensor([[-8.2897e+00,  1.7496e-01,  7.2001e-02,  9.3235e-02,  5.4107e-02,\n",
      "         -6.8465e-01, -3.0947e-01, -1.1942e+00,  1.4630e-01, -9.0115e+00,\n",
      "          4.8301e-01,  7.3672e-02, -9.8107e-02,  5.4660e-01, -3.4090e-01,\n",
      "         -6.7616e-01, -4.7719e-01, -4.4729e-01,  3.1100e-01, -2.0295e-01,\n",
      "          5.1492e+00, -5.8777e+00, -1.6171e+01, -5.3764e-02,  3.1310e-01,\n",
      "         -9.5140e-01, -2.0800e-01, -8.1406e-01,  8.1421e-01, -4.2590e-01,\n",
      "         -1.3113e+01, -4.1034e+00, -2.2928e-01, -5.4680e-01, -2.9158e-01,\n",
      "          9.5516e-03, -5.8937e+00,  6.2448e+00,  7.4299e+00,  6.4479e-01,\n",
      "          1.2986e+01, -3.0119e-01, -1.3159e+00,  8.3707e-01, -4.0182e-01,\n",
      "          6.1825e-01,  1.8422e+00,  5.9214e-01,  8.3773e-01, -4.6412e+00,\n",
      "         -5.5361e-01,  3.3430e-01, -1.4421e-01, -2.0272e-01, -5.4730e-02,\n",
      "         -2.9167e-01, -1.3918e+00, -3.3182e-02, -3.1946e+00,  6.8954e+00,\n",
      "         -8.4230e-01,  8.8980e-02,  2.2125e-01,  5.3875e-01, -1.1922e+00,\n",
      "         -1.3683e-01, -2.4759e+00,  3.7575e+00, -1.5510e+00, -1.8175e+00,\n",
      "          1.0258e+00,  8.7968e-01,  1.8428e+00,  4.9100e+00,  4.3622e-02,\n",
      "         -2.9034e+00, -2.3557e-01, -9.8301e-02, -2.4246e+00, -1.2080e+01,\n",
      "          1.8656e+00,  2.6907e-02, -1.4331e-02,  8.7462e-01, -4.6919e+00,\n",
      "         -7.7627e-02,  3.1448e+00,  1.4848e+00, -1.0065e-03, -4.8747e+00,\n",
      "          4.4226e-02, -1.2076e-01,  8.9760e-02, -2.6937e-01, -1.7042e+01,\n",
      "          2.4264e-02,  9.7473e-02, -2.6216e+00, -1.8956e-01,  3.0153e-01,\n",
      "         -3.5864e-01,  1.0407e+01,  1.9349e-01, -3.2199e-01,  1.1401e-01,\n",
      "          1.2060e-01, -1.7216e-02, -5.0201e+00,  2.8806e+00,  3.5066e-01,\n",
      "         -2.2399e-01,  4.5035e-01,  3.0974e-01, -1.7114e+01, -9.6955e-02,\n",
      "         -3.8356e+00, -2.6881e+00,  1.2476e+00,  1.2493e+00, -5.8233e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "FC3 tensor([[-6.7968,  2.0969,  8.4892,  1.5547, -3.6622, -6.4187,  2.6853]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "LOGITS tensor([[-1.5292e+01, -6.3980e+00, -5.6537e-03, -6.9401e+00, -1.2157e+01,\n",
      "         -1.4914e+01, -5.8095e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "    Acc.(Total): 100.00%%\n",
      "tensor([2.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IDX = 0\n",
    "MODE = \"secondary\"\n",
    "#plt.imshow(datasets[\"pitch\"][IDX][0].squeeze(), cmap=\"gray\")\n",
    "display_orig = datasets[MODE][IDX][0].squeeze() #datasets[MODE].X[IDX].squeeze()\n",
    "plt.imshow(display_orig, cmap=\"gray\")\n",
    "outputs, pred_y = validate(model[\"model\"][MODE], [datasets[MODE][idx][0] for idx in range(1)], [datasets[MODE][idx][1] for idx in range(46)])\n",
    "\n",
    "pred_y_old = pred_y\n",
    "print(pred_y)\n",
    "result = [transformations[\"class_to_annotation\"][MODE][int(cl)] for cl in pred_y]\n",
    "real = [transformations[\"class_to_annotation\"][MODE][int(cl)] for cl in [datasets[MODE][idx][1] for idx in range(46)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5da4904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating with\n",
      "    Acc.(Total): 97.83%%\n",
      "tensor([2., 0., 0., 3., 0., 0., 3., 2., 0., 2., 0., 0., 0., 0., 0., 2., 5., 0.,\n",
      "        3., 0., 0., 3., 5., 2., 2., 0., 0., 3., 0., 5., 2., 2., 0., 2., 0., 0.,\n",
      "        0., 0., 0., 3., 5., 0., 3., 0., 0., 2.])\n",
      "tensor([[2.0566e-05, 1.0744e-02, 9.6543e-01, 7.3362e-03, 1.8669e-04, 2.6836e-05,\n",
      "         1.6255e-02]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"./debug.pkl\", \"rb\") as file_handle:\n",
    "    image_list = pickle.load(file_handle)\n",
    "del image_list[24]\n",
    "display_img = image_list[IDX].squeeze()\n",
    "plt.imshow(display_img, cmap=\"gray\")\n",
    "outputs, pred_y_new = validate(model[\"model\"][MODE], image_list, [datasets[MODE][idx][1] for idx in range(46)])\n",
    "\n",
    "print(pred_y_new)\n",
    "\n",
    "result = [transformations[\"class_to_annotation\"][MODE][int(cl)] for cl in pred_y_new]\n",
    "    \n",
    "#print(outputs)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "20ab3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0., 0., 3., 0., 0., 3., 2., 0., 2., 0., 0., 0., 0., 0., 2., 5., 0.,\n",
      "        3., 0., 0., 3., 5., 2., 2., 0., 0., 3., 0., 5., 2., 2., 0., 2., 0., 0.,\n",
      "        0., 0., 0., 3., 5., 0., 3., 0., 0., 2.])\n",
      "tensor([2., 0., 0., 3., 0., 0., 3., 2., 0., 2., 0., 0., 0., 0., 0., 2., 5., 0.,\n",
      "        3., 0., 0., 3., 5., 2., 0., 2., 0., 0., 3., 0., 5., 2., 2., 0., 2., 0.,\n",
      "        0., 0., 0., 0., 3., 5., 0., 3., 0., 0.])\n",
      "tensor(67.3913)\n"
     ]
    }
   ],
   "source": [
    "print(pred_y_old)\n",
    "print(pred_y_new)\n",
    "\n",
    "print(sum(pred_y_old == pred_y_new)/len(pred_y_old)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7d9a6450",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48606/3403820627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./debug.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pitch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48606/2431932564.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, loaders)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f17b1d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pitch': [{'annotations': ['SHANG', 'GOU', 'FAN'],\n",
       "   'confidences': [0.9717637, 0.0213697, 0.0063694846]},\n",
       "  {'annotations': ['YI', 'SHANG', 'FAN'],\n",
       "   'confidences': [0.94845444, 0.036489867, 0.009354528]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'FAN'],\n",
       "   'confidences': [0.99979323, 0.00020591356, 7.941514e-07]},\n",
       "  {'annotations': ['SHANG', 'SI', 'FAN'],\n",
       "   'confidences': [0.61695075, 0.22039123, 0.12529738]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'CHE'],\n",
       "   'confidences': [0.93278456, 0.066941984, 0.00020123276]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'FAN'],\n",
       "   'confidences': [0.9999831, 1.6949274e-05, 1.1337127e-08]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'SI'],\n",
       "   'confidences': [0.87680596, 0.12162439, 0.0014622828]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'YI'],\n",
       "   'confidences': [0.91399336, 0.08417219, 0.00092003535]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'FAN'],\n",
       "   'confidences': [0.9999267, 7.277813e-05, 5.010857e-07]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'FAN'],\n",
       "   'confidences': [0.88645804, 0.07863538, 0.024378873]},\n",
       "  {'annotations': ['SHANG', 'FAN', 'LIU'],\n",
       "   'confidences': [0.5211432, 0.26535687, 0.13744481]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'HE'],\n",
       "   'confidences': [0.84760135, 0.14937188, 0.0029225892]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'SI'],\n",
       "   'confidences': [0.9962347, 0.0037466018, 1.7373803e-05]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'FAN'],\n",
       "   'confidences': [0.93123454, 0.06331526, 0.0051332666]},\n",
       "  {'annotations': ['SHANG', 'FAN', 'LIU'],\n",
       "   'confidences': [0.9999813, 1.531162e-05, 3.297668e-06]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'SI'],\n",
       "   'confidences': [0.9996897, 0.0002816584, 2.6498536e-05]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'HE'],\n",
       "   'confidences': [0.99982387, 0.00017592928, 1.7836153e-07]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'HE'],\n",
       "   'confidences': [0.8826559, 0.10694518, 0.00492396]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'SI'],\n",
       "   'confidences': [0.9247888, 0.07382105, 0.0007142197]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'HE'],\n",
       "   'confidences': [0.75581205, 0.24360734, 0.0005730134]},\n",
       "  {'annotations': ['SHANG', 'SI', 'FAN'],\n",
       "   'confidences': [0.7698903, 0.21661164, 0.010022262]},\n",
       "  {'annotations': ['SHANG', 'FAN', 'GOU'],\n",
       "   'confidences': [0.94795066, 0.021545503, 0.020271765]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'FAN'],\n",
       "   'confidences': [0.9999925, 7.44277e-06, 8.346969e-08]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'HE'],\n",
       "   'confidences': [0.57243866, 0.41268238, 0.006327622]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'SI'],\n",
       "   'confidences': [0.83181024, 0.16818792, 1.7504938e-06]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'HE'],\n",
       "   'confidences': [0.5261199, 0.46728134, 0.0053005414]},\n",
       "  {'annotations': ['LIU', 'CHE', 'SHANG'],\n",
       "   'confidences': [0.99952126, 0.00033659383, 0.00013068004]},\n",
       "  {'annotations': ['SHANG', 'LIU', 'FAN'],\n",
       "   'confidences': [0.997004, 0.002891712, 5.1902556e-05]},\n",
       "  {'annotations': ['LIU', 'SHANG', 'YI'],\n",
       "   'confidences': [0.76322436, 0.2346619, 0.0015641846]},\n",
       "  {'annotations': ['SHANG', 'SI', 'LIU'],\n",
       "   'confidences': [0.9802036, 0.01529276, 0.003626238]}],\n",
       " 'secondary': [{'annotations': ['YE', None, 'DING_ZHU'],\n",
       "   'confidences': [0.73037374, 0.23708701, 0.02901647]},\n",
       "  {'annotations': ['ZHE', 'DING_ZHU', 'YE'],\n",
       "   'confidences': [0.9965287, 0.0031852017, 0.00028606664]},\n",
       "  {'annotations': ['YE', 'ZHE', None],\n",
       "   'confidences': [0.8054442, 0.14703423, 0.04710453]},\n",
       "  {'annotations': ['DING_ZHU', None, 'YE'],\n",
       "   'confidences': [0.594868, 0.23690642, 0.15546918]},\n",
       "  {'annotations': ['DING_ZHU', 'YE', 'ZHE'],\n",
       "   'confidences': [0.48920915, 0.44915092, 0.060059085]},\n",
       "  {'annotations': ['YE', None, 'ZHE'],\n",
       "   'confidences': [0.935173, 0.046103466, 0.01854845]},\n",
       "  {'annotations': [None, 'YE', 'ZHE'],\n",
       "   'confidences': [0.5181913, 0.3148642, 0.15888582]},\n",
       "  {'annotations': ['ZHE', 'YE', None],\n",
       "   'confidences': [0.5106367, 0.33897305, 0.10431484]},\n",
       "  {'annotations': ['YE', None, 'ZHE'],\n",
       "   'confidences': [0.6159845, 0.33553827, 0.045829553]},\n",
       "  {'annotations': ['YE', 'ZHE', 'XIAO_ZHU'],\n",
       "   'confidences': [0.5026576, 0.47938675, 0.013492687]},\n",
       "  {'annotations': [None, 'YE', 'ZHE'],\n",
       "   'confidences': [0.51067096, 0.2685888, 0.20090371]},\n",
       "  {'annotations': [None, 'YE', 'ZHE'],\n",
       "   'confidences': [0.53715557, 0.44587576, 0.00842176]},\n",
       "  {'annotations': [None, 'ZHE', 'YE'],\n",
       "   'confidences': [0.43628982, 0.39029208, 0.16301848]},\n",
       "  {'annotations': ['YE', 'DING_ZHU', 'DA_ZHU'],\n",
       "   'confidences': [0.714484, 0.2706672, 0.009204592]},\n",
       "  {'annotations': ['YE', None, 'DING_ZHU'],\n",
       "   'confidences': [0.95409834, 0.023796378, 0.021801341]},\n",
       "  {'annotations': ['YE', None, 'ZHE'],\n",
       "   'confidences': [0.76733726, 0.214108, 0.017548788]},\n",
       "  {'annotations': ['YE', 'ZHE', None],\n",
       "   'confidences': [0.9754489, 0.012504978, 0.011805194]},\n",
       "  {'annotations': ['DING_ZHU', 'YE', 'ZHE'],\n",
       "   'confidences': [0.92401856, 0.057122584, 0.018833319]},\n",
       "  {'annotations': [None, 'YE', 'ZHE'],\n",
       "   'confidences': [0.7488238, 0.16353485, 0.06867286]},\n",
       "  {'annotations': ['YE', 'DING_ZHU', None],\n",
       "   'confidences': [0.9503968, 0.039693028, 0.0049546678]},\n",
       "  {'annotations': [None, 'DING_ZHU', 'YE'],\n",
       "   'confidences': [0.64144397, 0.22639129, 0.1286729]},\n",
       "  {'annotations': [None, 'YE', 'ZHE'],\n",
       "   'confidences': [0.6079127, 0.38730675, 0.003569429]},\n",
       "  {'annotations': ['YE', 'ZHE', None],\n",
       "   'confidences': [0.9603847, 0.021989373, 0.017355436]},\n",
       "  {'annotations': ['ZHE', 'DING_ZHU', 'YE'],\n",
       "   'confidences': [0.9749147, 0.02422835, 0.00077789824]},\n",
       "  {'annotations': ['YE', 'ZHE', 'DING_ZHU'],\n",
       "   'confidences': [0.9355137, 0.034184955, 0.018552376]},\n",
       "  {'annotations': ['YE', 'DING_ZHU', None],\n",
       "   'confidences': [0.9301806, 0.033858657, 0.01746448]},\n",
       "  {'annotations': [None, 'YE', 'DING_ZHU'],\n",
       "   'confidences': [0.9323719, 0.05664613, 0.0057905675]},\n",
       "  {'annotations': ['YE', None, 'ZHE'],\n",
       "   'confidences': [0.65487814, 0.34240365, 0.0021608267]},\n",
       "  {'annotations': ['ZHE', 'YE', 'DING_ZHU'],\n",
       "   'confidences': [0.9024044, 0.09235956, 0.005218917]},\n",
       "  {'annotations': ['ZHE', 'YE', 'DING_ZHU'],\n",
       "   'confidences': [0.6391554, 0.35081348, 0.009609551]}]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_notation(image_list, model, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d005c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/.local/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_similar(image_list, model, transformations)[\"pitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b19badc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f0694c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[-16.4035, -16.4035, -16.4035,  ..., -16.4035, -16.4035, -16.4035],\n",
       "          [-16.4035, -16.4035, -16.4035,  ..., -16.4035, -16.4035, -16.4035],\n",
       "          [-16.4035, -16.4035, -16.4035,  ..., -16.4035, -16.4035, -16.4035],\n",
       "          ...,\n",
       "          [-16.4035, -16.4035, -16.4035,  ..., -16.4035, -16.4035, -16.4035],\n",
       "          [-16.4035, -16.4035, -16.4035,  ..., -16.4035, -16.4035, -16.4035],\n",
       "          [-16.4035, -16.4035, -16.4035,  ..., -16.4035, -16.4035, -16.4035]]]),\n",
       " 'edition': 'siku',\n",
       " 'distance': 0.06930497533192405,\n",
       " 'similarity': 14.428978514322745,\n",
       " 'annotation': 'SHANG'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29462ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
